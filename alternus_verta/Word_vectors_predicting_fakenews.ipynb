{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Swetha Chandrasekar\n",
    "# 012497628\n",
    "# Exploring the use of word vectors and doc vectors as features for predicting fake news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>Business problem </u>\n",
    "<h1> Given a text of statement made by a politician and the party affliation, predict whether the news is fake news or not </h1>\n",
    "<h2> dataset: LIAR dataset </h2>\n",
    "<h3>labelling <br>\n",
    "\n",
    "<font color = red>0 - news labelled as [ false, barely true ] </font> <b> in LIAR dataset </b> <br>\n",
    "<font color=green> 1 - new labelled as [true, mostly true, half true] <b> in LIAR dataset </b></font>\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = open('train.tsv','r',encoding='utf-8').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10269"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Size of the dataset LIAR\n",
    "len(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2635.json\n",
      "1 false\n",
      "2 Says the Annies List political group supports third-trimester abortions on demand.\n",
      "3 abortion\n",
      "4 dwayne-bohac\n",
      "5 State representative\n",
      "6 Texas\n",
      "7 republican\n",
      "8 0\n",
      "9 1\n",
      "10 0\n",
      "11 0\n",
      "12 0\n",
      "13 a mailer\n"
     ]
    }
   ],
   "source": [
    "d =lines[0].strip().split('\\t')\n",
    "for i in range(0,len(d)):\n",
    "    print (i,d[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Label 1 for true , Label 0 for fake news </h1>\n",
    "<h2> true and mostly-true news are marked as 1 and false, barely-true and half-true are marked as 0 </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_data = []\n",
    "false_data = []\n",
    "overall_data = []\n",
    "label = []\n",
    "for i in range(0,len(lines)):\n",
    "    line = lines[i].strip().lower().split()\n",
    "    overall_data.append(lines[i].strip().lower())\n",
    "    if (line[1]== 'true' or line[1] =='mostly-true'):\n",
    "        true_data.append(lines[i].strip().lower())\n",
    "        label.append(1)\n",
    "    else:\n",
    "        false_data.append(lines[i].strip().lower())\n",
    "        label.append(0)\n",
    "## Please note overall data still contains all of training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "1. word tokenization\n",
    "2. stop words removal\n",
    "3. punctuation removal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>use nltk word tokenize and remove stop words </h1>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['republican',\n",
       " 'says',\n",
       " 'annies',\n",
       " 'list',\n",
       " 'political',\n",
       " 'group',\n",
       " 'supports',\n",
       " 'third-trimester',\n",
       " 'abortions',\n",
       " 'demand',\n",
       " 'republican-says',\n",
       " 'says-annies',\n",
       " 'annies-list',\n",
       " 'list-political',\n",
       " 'political-group',\n",
       " 'group-supports',\n",
       " 'supports-third-trimester',\n",
       " 'third-trimester-abortions',\n",
       " 'abortions-demand']"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##use nltk word tokenize and remove stop words\n",
    "data = []\n",
    "for line in overall_data:\n",
    "    text = line.split('\\t')[2].lower()\n",
    "    party = line.split('\\t')[7].lower()\n",
    "    line = party + ' ' + text \n",
    "    tokens = [i for i in word_tokenize(line.strip().lower()) if i not in stop]\n",
    "    bigrams = []\n",
    "    for i in range(0,len(tokens)-1):\n",
    "        bigram = tokens[i]+'-'+tokens[i+1]\n",
    "        bigrams.append(bigram)\n",
    "    for bigram in bigrams:\n",
    "        tokens.append(bigram)\n",
    "        \n",
    "    data.append(tokens)\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Use Gensim and create word vectors from the dataset </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=92977, size=100, alpha=0.025)\n",
      "[-0.11455256 -0.2024197  -0.20666882  0.25210455  0.28490305 -0.2682967\n",
      "  0.37496385 -0.26639718  0.32168755 -0.07837708 -0.5220979  -0.08559928\n",
      "  0.18434428  0.22665851 -0.00426131  0.25039798  0.1059581  -0.26667228\n",
      "  0.35788766 -0.5954613  -0.01996064  0.03948819  0.15410815  0.25189525\n",
      "  0.05272826 -0.01522747 -0.336694   -0.2888242  -0.05908775  0.3316827\n",
      " -0.16743724 -0.3150222  -0.04912709  0.15707426  0.10019339  0.1446167\n",
      " -0.7101526   0.22032574 -0.0217316   0.2865798  -0.28632015 -0.35354972\n",
      "  0.58226573  0.5503629   0.41201013 -0.14536437  0.00269027 -0.5509862\n",
      " -0.22852524  0.01167944 -0.01227845 -0.04038278  0.2171643   0.31045085\n",
      "  0.13307571 -0.17424417 -0.16017133  0.07239676  0.22099054 -0.14130394\n",
      "  0.56915617 -0.32427248 -0.2148879  -0.22502768 -0.47478056  0.1860454\n",
      "  0.3319483   0.29702356 -0.04012456  0.11645429  0.54083854  0.03426909\n",
      "  0.46273822  0.06835679 -0.17880689  0.06672256 -0.35904837  0.32273993\n",
      "  0.43037656 -0.48949093  0.09068238  0.41939086  0.1321773   0.46757564\n",
      "  0.07915614 -0.00620449  0.33656335 -0.03281384 -0.30268767  0.1167055\n",
      "  0.18038371  0.27401805  0.15664218 -0.6212016   0.16296092  0.09980382\n",
      "  0.14157702  0.13893105 -0.22166869 -0.30903915]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=92977, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "# code referenced from https://machinelearningmastery.com/develop-word-embeddings-python-gensim/\n",
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(data, min_count=1)\n",
    "# summarize the loaded model\n",
    "print(model)\n",
    "# summarize vocabulary\n",
    "words = list(model.wv.vocab)\n",
    "#print(words)\n",
    "# access vector for one word\n",
    "print(model['democrat'])\n",
    "# save model\n",
    "model.save('overall_model.bin')\n",
    "# load model\n",
    "overall_model = Word2Vec.load('overall_model.bin')\n",
    "print(overall_model)\n",
    "word_vector = overall_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.59867144,  0.01503648, -0.77621317, -0.19520019,  0.80228025,\n",
       "       -0.41991404,  0.09400711, -0.07046393,  0.49680176,  0.3178129 ,\n",
       "       -0.18584426, -0.04655198, -0.06158256,  0.33365053,  0.4838549 ,\n",
       "        0.08842117,  0.47025567, -0.44300038,  0.68935573, -0.7434183 ,\n",
       "       -0.10760821, -0.16961616, -0.40767553,  0.6566415 , -0.0762207 ,\n",
       "        0.2985728 , -0.7110756 , -0.49166057, -0.6082097 ,  0.29672405,\n",
       "       -0.28791398, -0.18597479,  0.71231675, -0.23035446,  0.32295   ,\n",
       "        0.2515094 , -0.8740319 ,  0.71074235, -0.48459288,  0.58007246,\n",
       "       -0.3631735 ,  0.0080174 ,  0.8379245 ,  0.44353825,  0.6020888 ,\n",
       "       -0.14349203, -0.35740232, -0.10585239, -0.6955374 , -0.58631325,\n",
       "        0.30091035, -0.42195475,  0.2943132 ,  0.95403713,  0.2684464 ,\n",
       "        0.28492817,  0.67836344,  0.34067777, -0.47441575, -0.40000623,\n",
       "        0.26841143, -0.7904903 , -0.6389411 , -0.563384  , -0.15640144,\n",
       "        0.02775991,  0.88779026,  0.6993233 ,  0.04734593, -0.15120322,\n",
       "        0.8662171 ,  0.0069867 ,  0.5180113 ,  0.6023444 , -0.15279415,\n",
       "        0.33547676, -0.08903733,  0.90076125,  0.44153348, -0.20382753,\n",
       "        0.06291965, -0.0408917 ,  0.5292915 ,  0.75738883,  0.28924033,\n",
       "        0.35621807,  0.27090904, -0.06643717, -0.82351273,  0.37117183,\n",
       "        0.47223964,  0.67017514, -0.02718855, -0.72816044,  0.59650546,\n",
       "        0.06822478,  0.33623928, -0.03455564, -0.30168927,  0.0356487 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vector[\"obama\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word vectors represent latent signals projecting them as vectors in an n-dimensional space\n",
    "## Let us use word vectors directly as features to predict if the news is fake or not\n",
    "word vector simply gives array of numbers of size n. <br> <b>\n",
    "<font color=green>We can directly use the sum of word vectors as features.</font> <br> </b>\n",
    "## 1. sentences are made of multiple words, \n",
    "## 2. then we simply add the vectors of each word and get resultant vector for that sentence and use that as feature\n",
    "## example vector('obama is socialist') = vector['obama'] + vector['is']+  vector['socialist']\n",
    "## <font color=red> This may not be the right approach as doc2vec is a better approach as suggested by professor in class. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  \"\"\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "for line in data:\n",
    "    vectors = []\n",
    "    for word in line:\n",
    "        if word in overall_model:\n",
    "            vector = overall_model[word]\n",
    "            vectors.append(vector)\n",
    "    word_vector = []\n",
    "\n",
    "    for i in range(0,len(vectors[0])):\n",
    "        val = 0.0\n",
    "        for j in range(0,len(vectors)):\n",
    "            val += vectors[j][i]\n",
    "        word_vector.append(val)\n",
    "    X.append(word_vector)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Logistic regression on features from word vector to label (0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=1, warm_start=False)\n",
      "Score: 0.6436222005842259\n",
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "X_df = pd.DataFrame(X)\n",
    "Y = pd.DataFrame(label)\n",
    "scaled_X = X_df\n",
    "scaled_Y = Y\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_X, scaled_Y, test_size=0.3, random_state=0)\n",
    "lm = linear_model.LogisticRegression(verbose=1)\n",
    "model = lm.fit(X_train, y_train)\n",
    "print (model)\n",
    "predictions = lm.predict(X_test)\n",
    "\n",
    "print (\"Score:\", model.score(X_test, y_test))\n",
    "w2vecmodel = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression was trained on word vectors and it has accuracy of 0.64 in predicting the fake news on test set. let us test on sample sentence \n",
    "lets say if republican says \"obama supports third trimester abortion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing= \"republican says obama supports third trimester abortion\"\n",
    "\n",
    "import numpy as np\n",
    "def getVector(s):\n",
    "    s = s.lower()\n",
    "    words = word_tokenize(s)\n",
    "    vectors = []\n",
    "    for word in words:\n",
    "        if word in overall_model:\n",
    "            vectors.append(overall_model[word])\n",
    "    sum_vector = []\n",
    "    for i in range(0,len(vectors[0])):\n",
    "        val = 0.0\n",
    "        for j in range(0,len(vectors)):\n",
    "            val += vectors[j][i]\n",
    "        sum_vector.append(val)\n",
    "    return pd.DataFrame([sum_vector])\n",
    "\n",
    "testX = getVector(testing)\n",
    "result_score = model.predict(testX)\n",
    "model.predict_proba(testX)\n",
    "### 0.71 probability that news belong to class 0 which is fake news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class 1 probability is 0.28 and class 0 probability is 0.72 , so its likely a <font color=red> fake news </font> that <font color=red> \"republican says obama supports third trimester abortion\" </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So far, we tried word2vec and summed the word vectors of individual words to get vector for each text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's try doc2vec to see if embeddings for text as a whole is performs better than word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'republican says the annies list political group supports third-trimester abortions on demand.'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://medium.com/@mishra.thedeepak/doc2vec-simple-implementation-example-df2afbbfbad5\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "data = []\n",
    "for line in overall_data:\n",
    "    T = line.strip().split('\\t')\n",
    "    text = T[2]\n",
    "    party = T[7]\n",
    "    text = party + ' ' + text\n",
    "    data.append(text.lower())\n",
    "tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(data)]\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/gensim/models/doc2vec.py:570: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:17: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1\n",
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n",
      "iteration 5\n",
      "iteration 6\n",
      "iteration 7\n",
      "iteration 8\n",
      "iteration 9\n",
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 10\n",
    "vec_size = 20\n",
    "alpha = 0.025\n",
    "\n",
    "model = Doc2Vec(size=vec_size,\n",
    "                alpha=alpha, \n",
    "                min_alpha=0.00025,\n",
    "                min_count=1,\n",
    "                dm =1)\n",
    "  \n",
    "model.build_vocab(tagged_data)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    print('iteration {0}'.format(epoch))\n",
    "    model.train(tagged_data,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.iter)\n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha\n",
    "\n",
    "model.save(\"d2v.model\")\n",
    "print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.16354604,  0.07839309,  0.05091531,  0.02476311, -0.05669187,\n",
       "       -0.05538713,  0.01073092, -0.10960696,  0.08444428, -0.27399284,\n",
       "        0.03156104, -0.08877857, -0.22538221, -0.030525  ,  0.03771114,\n",
       "        0.15414655, -0.22142938,  0.15237255, -0.09798911, -0.01141477],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "\n",
    "model= Doc2Vec.load(\"d2v.model\")\n",
    "#to find the vector of a document which is not in training data\n",
    "test_data = word_tokenize(\"republicans say obama supports third trimester abortion\".lower())\n",
    "v1 = model.infer_vector(test_data)\n",
    "v1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vecFeatures = []\n",
    "for line in overall_data:\n",
    "    T = line.strip().split('\\t')\n",
    "    text = T[2]\n",
    "    party = T[7]\n",
    "    text = party + ' ' + text\n",
    "    doc2vecFeatures.append(model.infer_vector(word_tokenize(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let us use doc2vec vector embeddings directly as a feature and measure its performance against word2vec vector sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=1, warm_start=False)\n",
      "Score: 0.6403765011359948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "X_df = pd.DataFrame(doc2vecFeatures)\n",
    "Y = pd.DataFrame(label)\n",
    "scaled_X = X_df\n",
    "scaled_Y = Y\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_X, scaled_Y, test_size=0.3, random_state=0)\n",
    "lm = linear_model.LogisticRegression(verbose=1)\n",
    "model = lm.fit(X_train, y_train)\n",
    "print (model)\n",
    "predictions = lm.predict(X_test)\n",
    "\n",
    "print (\"Score:\", model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"republican says obama supports third trimester abortion\"\n",
    "dvmodel= Doc2Vec.load(\"d2v.model\")\n",
    "test_sentence= [dvmodel.infer_vector(word_tokenize(text))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(pd.DataFrame(test_sentence))\n",
    "predicted_probabilities = model.predict_proba(pd.DataFrame(test_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec prediction\n",
      "=====================\n",
      "Predicted class: [0] Predicted probabilities for class[0,1]: [[0.63788689 0.36211311]]\n",
      "Length of embeddings from Doc2Vec: 20\n",
      "Length of embeddings from Word2Vec embeddings: 100\n",
      "====================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  if __name__ == '__main__':\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "print (\"Doc2Vec prediction\")\n",
    "print (\"=====================\")\n",
    "print (\"Predicted class:\",predicted, \"Predicted probabilities for class[0,1]:\",predicted_probabilities)\n",
    "print (\"Length of embeddings from Doc2Vec:\", len(dvmodel.infer_vector(word_tokenize(text))))\n",
    "df = getVector(text)\n",
    "print (\"Length of embeddings from Word2Vec embeddings:\",len(df.columns))\n",
    "print (\"====================================================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class 1 probability is 0.35 and class 0 probability is 0.64 , so its likely a <font color=red> fake news </font> that <font color=red> \"republican says obama supports third trimester abortion\" </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy in predicting fake news based on \n",
    "## Doc2Vec embeddings: 0.6403\n",
    "##  Word2Vec embeddings: 0.643"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec embeddings with vector summation seems to perform similar to Doc2Vec embeddings\n",
    "# it could be because word2vec is generating vectors of size <u>100</u> and while doc2vec is generating vectors of size <u>20.</u> \n",
    "# Doc 2 vec seem to get the same accuracy with 20 embeddings that word2vec does with 100 embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
