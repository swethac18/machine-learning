{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Swetha Chandrasekar\n",
    "# 012497628\n",
    "# Exploring the use of word vectors and doc vectors as features for predicting fake news\n",
    "# <u>Business problem </u>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Given a 'text' of statement made by a politician and 'the party affliation', predict whether the news is fake news or not </h1>\n",
    "<h2> dataset: LIAR dataset </h2>\n",
    "<h3>labelling <br>\n",
    "\n",
    "<font color = red>0 - news labelled as [ false, barely true ] </font> <b> in LIAR dataset </b> <br>\n",
    "<font color=green> 1 - new labelled as [true, mostly true, half true] <b> in LIAR dataset </b></font>\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = open('train.tsv','r',encoding='utf-8').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10269"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Size of the dataset LIAR\n",
    "len(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2635.json\n",
      "1 false\n",
      "2 Says the Annies List political group supports third-trimester abortions on demand.\n",
      "3 abortion\n",
      "4 dwayne-bohac\n",
      "5 State representative\n",
      "6 Texas\n",
      "7 republican\n",
      "8 0\n",
      "9 1\n",
      "10 0\n",
      "11 0\n",
      "12 0\n",
      "13 a mailer\n"
     ]
    }
   ],
   "source": [
    "d =lines[0].strip().split('\\t')\n",
    "for i in range(0,len(d)):\n",
    "    print (i,d[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Label 1 for true , Label 0 for fake news </h1>\n",
    "<h2> true and mostly-true news are marked as 1 and false, barely-true and half-true are marked as 0 </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_data = []\n",
    "false_data = []\n",
    "overall_data = []\n",
    "label = []\n",
    "for i in range(0,len(lines)):\n",
    "    line = lines[i].strip().lower().split()\n",
    "    overall_data.append(lines[i].strip().lower())\n",
    "    if (line[1]== 'true' or line[1] =='mostly-true'):\n",
    "        true_data.append(lines[i].strip().lower())\n",
    "        label.append(1)\n",
    "    else:\n",
    "        false_data.append(lines[i].strip().lower())\n",
    "        label.append(0)\n",
    "## Please note overall data still contains all of training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "1. word tokenization\n",
    "2. stop words removal\n",
    "3. punctuation removal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>use nltk word tokenize and remove stop words </h1>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['republican',\n",
       " 'says',\n",
       " 'annies',\n",
       " 'list',\n",
       " 'political',\n",
       " 'group',\n",
       " 'supports',\n",
       " 'third-trimester',\n",
       " 'abortions',\n",
       " 'demand',\n",
       " 'republican-says',\n",
       " 'says-annies',\n",
       " 'annies-list',\n",
       " 'list-political',\n",
       " 'political-group',\n",
       " 'group-supports',\n",
       " 'supports-third-trimester',\n",
       " 'third-trimester-abortions',\n",
       " 'abortions-demand']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##use nltk word tokenize and remove stop words\n",
    "import nltk;\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "stop = stopwords.words('english') + list(string.punctuation)\n",
    "data = []\n",
    "for line in overall_data:\n",
    "    text = line.split('\\t')[2].lower()\n",
    "    party = line.split('\\t')[7].lower()\n",
    "    line = party + ' ' + text \n",
    "    tokens = [i for i in word_tokenize(line.strip().lower()) if i not in stop]\n",
    "    bigrams = []\n",
    "    for i in range(0,len(tokens)-1):\n",
    "        bigram = tokens[i]+'-'+tokens[i+1]\n",
    "        bigrams.append(bigram)\n",
    "    for bigram in bigrams:\n",
    "        tokens.append(bigram)\n",
    "        \n",
    "    data.append(tokens)\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Use Gensim to create word vectors from the dataset </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=92977, size=100, alpha=0.025)\n",
      "[ 0.3042278  -0.19084701  0.30704698 -0.04614774  0.30127057 -0.39753827\n",
      "  0.21099289  0.04760346 -0.09727817  0.00110407 -0.1509957  -0.58583367\n",
      "  0.19755447  0.15945695  0.02719315 -0.05512587 -0.4164396   0.7089397\n",
      " -0.10720449  0.2343237   0.10276304  0.29957622  0.44162408 -0.05149297\n",
      "  0.07815396 -0.3893264   0.10842299 -0.2528649  -0.55113417  0.21559718\n",
      " -0.35969508 -0.0623625  -0.1495717  -0.43451336  0.09172938  0.15576635\n",
      "  0.03578302  0.5117066  -0.9358748   0.01989187 -0.0129511  -0.09246799\n",
      "  0.04944352  0.16114403  0.34654868 -0.22775806 -0.01824609 -0.21470852\n",
      "  0.41621545  0.295388   -0.17595226  0.01865412  0.4773739  -0.21952885\n",
      "  0.00775523 -0.00489549 -0.03777182 -0.07143733 -0.0597306   0.23876062\n",
      " -0.05098007  0.30073833 -0.40168673  0.08237503  0.17868659 -0.12636824\n",
      " -0.25743777 -0.21753372 -0.16529737  0.4817043   0.5583272   0.00732034\n",
      "  0.00345164 -0.05041166 -0.20952019  0.33536816 -0.25522247  0.54077387\n",
      "  0.30054918  0.46637705 -0.00419089  0.29754534 -0.0015635   0.19734138\n",
      " -0.25154206 -0.77373713 -0.2623047  -0.13793403  0.05970129 -0.35036823\n",
      " -0.12240653 -0.27497897 -0.1606711  -0.20746452 -0.08332076  0.23526283\n",
      "  0.06022962  0.2639184  -0.08299077  0.79198265]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=92977, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "# code referenced from https://machinelearningmastery.com/develop-word-embeddings-python-gensim/\n",
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(data, min_count=1)\n",
    "# summarize the loaded model\n",
    "print(model)\n",
    "# summarize vocabulary\n",
    "words = list(model.wv.vocab)\n",
    "#print(words)\n",
    "# access vector for one word\n",
    "print(model['democrat'])\n",
    "# save model\n",
    "model.save('overall_model.bin')\n",
    "# load model\n",
    "overall_model = Word2Vec.load('overall_model.bin')\n",
    "print(overall_model)\n",
    "word_vector = overall_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.2853234 , -0.48534012,  0.5000886 , -0.2689489 ,  0.7408342 ,\n",
       "       -0.5546253 ,  0.3838437 , -0.14750402,  0.15514989,  0.0542842 ,\n",
       "        0.47308603, -0.7753996 ,  0.22022419,  0.392062  ,  0.44076765,\n",
       "       -0.3084322 , -0.37180814,  1.0051496 , -0.4380037 , -0.544555  ,\n",
       "        0.53491664,  0.5341076 ,  0.8476037 , -0.16393945,  0.3179823 ,\n",
       "       -0.6324809 ,  0.82084453,  0.09179328, -1.0027754 ,  0.15624635,\n",
       "       -0.14643313,  0.25749782, -0.3352869 , -0.89560634,  0.18808894,\n",
       "        0.32283223, -0.53243345,  0.91719013, -1.1129243 , -0.15304033,\n",
       "        0.64297837,  0.11762323, -0.35534397,  0.38519317,  0.36889327,\n",
       "       -0.48139963, -0.5959327 , -0.5698601 ,  0.03823274,  0.46288544,\n",
       "       -0.18475063, -0.08369005,  0.46727866, -0.2556913 , -0.04049212,\n",
       "        0.28839445, -0.20834452,  0.02781974, -0.40480906, -0.00590318,\n",
       "        0.21219815, -0.017327  , -0.3677732 , -0.47276872,  0.3141341 ,\n",
       "        0.47274998, -0.3129866 , -0.519324  , -0.81234276,  0.51819575,\n",
       "        0.71919924,  0.07818636,  0.35480076,  0.09937452, -0.32342872,\n",
       "        0.0867957 ,  0.24416296,  0.7707098 ,  0.4324174 ,  0.5352922 ,\n",
       "       -0.09431642,  0.2765572 ,  0.7005047 ,  0.46443093, -0.56465364,\n",
       "       -1.0676458 , -0.06024292,  0.14328232,  0.3975109 , -0.20328468,\n",
       "        0.12947676, -0.154886  ,  0.2842232 , -0.26254994, -0.4477695 ,\n",
       "        0.6120097 ,  0.42865163,  0.23175946, -0.66272795,  0.76864135],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vector[\"obama\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word vectors represent latent signals projecting word as a vector in an n-dimensional space\n",
    "## Let us use word vectors directly as features to predict if the news is fake or not\n",
    "word vector simply gives array of numbers of size n. <br> <b>\n",
    "<font color=green>We can directly use the sum of word vectors as features.</font> <br> </b>\n",
    "## 1. sentences are made of multiple words, \n",
    "## 2. then we simply add the vectors of each word and get resultant vector for that sentence and use that as feature\n",
    "## example vector('obama is socialist') = vector['obama'] + vector['is']+  vector['socialist']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> <font color=red> This may not be the right approach as doc2vec is a better approach. This was suggested by professor in class to compare it with doc2vec. </font> </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  \"\"\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "for line in data:\n",
    "    vectors = []\n",
    "    for word in line:\n",
    "        if word in overall_model:\n",
    "            vector = overall_model[word]\n",
    "            vectors.append(vector)\n",
    "    word_vector = []\n",
    "\n",
    "    for i in range(0,len(vectors[0])):\n",
    "        val = 0.0\n",
    "        for j in range(0,len(vectors)):\n",
    "            val += vectors[j][i]\n",
    "        word_vector.append(val)\n",
    "    X.append(word_vector)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Logistic regression on features from word vector to predict label (0,1)\n",
    "# 0 is fake news, 1 is truthful news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=1, warm_start=False)\n",
      "Score: 0.6416747809152873\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "X_df = pd.DataFrame(X)\n",
    "Y = pd.DataFrame(label)\n",
    "scaled_X = X_df\n",
    "scaled_Y = Y\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_X, scaled_Y, test_size=0.3, random_state=0)\n",
    "lm = linear_model.LogisticRegression(verbose=1)\n",
    "model = lm.fit(X_train, y_train)\n",
    "print (model)\n",
    "predictions = lm.predict(X_test)\n",
    "\n",
    "print (\"Score:\", model.score(X_test, y_test))\n",
    "w2vecmodel = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression was trained on word vectors and it has accuracy of 0.64 in predicting the fake news on test set. let us test on a sample sentence \n",
    "# lets say text= \"republican says obama supports third trimester abortion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  if __name__ == '__main__':\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.75521006, 0.24478994]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing= \"republican says obama supports third trimester abortion\"\n",
    "\n",
    "import numpy as np\n",
    "def getVector(s):\n",
    "    s = s.lower()\n",
    "    words = word_tokenize(s)\n",
    "    vectors = []\n",
    "    for word in words:\n",
    "        if word in overall_model:\n",
    "            vectors.append(overall_model[word])\n",
    "    sum_vector = []\n",
    "    for i in range(0,len(vectors[0])):\n",
    "        val = 0.0\n",
    "        for j in range(0,len(vectors)):\n",
    "            val += vectors[j][i]\n",
    "        sum_vector.append(val)\n",
    "    return pd.DataFrame([sum_vector])\n",
    "\n",
    "testX = getVector(testing)\n",
    "result_score = model.predict(testX)\n",
    "model.predict_proba(testX)\n",
    "### 0.71 probability that news belong to class 0 which is fake news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class 1 probability is 0.24 and class 0 probability is 0.75 , \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>so its likely fake</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><font color=red> fake news: </font>  <font color=red> \"republican says obama supports third trimester abortion\" </font> </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So far, we tried word2vec and summed the word vectors of individual words to get vector for each text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's try doc2vec to see if embeddings for text as a whole is performs better than word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'republican says the annies list political group supports third-trimester abortions on demand.'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://medium.com/@mishra.thedeepak/doc2vec-simple-implementation-example-df2afbbfbad5\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "data = []\n",
    "for line in overall_data:\n",
    "    T = line.strip().split('\\t')\n",
    "    text = T[2]\n",
    "    party = T[7]\n",
    "    text = party + ' ' + text\n",
    "    data.append(text.lower())\n",
    "tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(data)]\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/gensim/models/doc2vec.py:570: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:17: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1\n",
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n",
      "iteration 5\n",
      "iteration 6\n",
      "iteration 7\n",
      "iteration 8\n",
      "iteration 9\n",
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 10\n",
    "vec_size = 20\n",
    "alpha = 0.025\n",
    "\n",
    "model = Doc2Vec(size=vec_size,\n",
    "                alpha=alpha, \n",
    "                min_alpha=0.00025,\n",
    "                min_count=1,\n",
    "                dm =1)\n",
    "  \n",
    "model.build_vocab(tagged_data)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    print('iteration {0}'.format(epoch))\n",
    "    model.train(tagged_data,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.iter)\n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha\n",
    "\n",
    "model.save(\"d2v.model\")\n",
    "print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00403764, -0.07682121, -0.01614466, -0.0349686 , -0.04510788,\n",
       "       -0.06150846,  0.18929797,  0.01581638,  0.02578834,  0.19873458,\n",
       "        0.20016974, -0.24335104,  0.11643776,  0.09796631, -0.13430543,\n",
       "        0.13469905,  0.01366164, -0.07000271,  0.06492338,  0.19003768],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "\n",
    "model= Doc2Vec.load(\"d2v.model\")\n",
    "#to find the vector of a document which is not in training data\n",
    "test_data = word_tokenize(\"republicans say obama supports third trimester abortion\".lower())\n",
    "v1 = model.infer_vector(test_data)\n",
    "v1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vecFeatures = []\n",
    "for line in overall_data:\n",
    "    T = line.strip().split('\\t')\n",
    "    text = T[2]\n",
    "    party = T[7]\n",
    "    text = party + ' ' + text\n",
    "    doc2vecFeatures.append(model.infer_vector(word_tokenize(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let us use doc2vec vector embeddings directly as a feature and measure its performance against word2vec vector sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=1, warm_start=False)\n",
      "Score: 0.6410256410256411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "X_df = pd.DataFrame(doc2vecFeatures)\n",
    "Y = pd.DataFrame(label)\n",
    "scaled_X = X_df\n",
    "scaled_Y = Y\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_X, scaled_Y, test_size=0.3, random_state=0)\n",
    "lm = linear_model.LogisticRegression(verbose=1)\n",
    "model = lm.fit(X_train, y_train)\n",
    "print (model)\n",
    "predictions = lm.predict(X_test)\n",
    "\n",
    "print (\"Score:\", model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"republican says obama supports third trimester abortion\"\n",
    "dvmodel= Doc2Vec.load(\"d2v.model\")\n",
    "test_sentence= [dvmodel.infer_vector(word_tokenize(text))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(pd.DataFrame(test_sentence))\n",
    "predicted_probabilities = model.predict_proba(pd.DataFrame(test_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec prediction\n",
      "=====================\n",
      "Predicted class: [0] Predicted probabilities for class[0,1]: [[0.63726274 0.36273726]]\n",
      "Length of embeddings from Doc2Vec: 20\n",
      "Length of embeddings from Word2Vec embeddings: 100\n",
      "====================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  if __name__ == '__main__':\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "print (\"Doc2Vec prediction\")\n",
    "print (\"=====================\")\n",
    "print (\"Predicted class:\",predicted, \"Predicted probabilities for class[0,1]:\",predicted_probabilities)\n",
    "print (\"Length of embeddings from Doc2Vec:\", len(dvmodel.infer_vector(word_tokenize(text))))\n",
    "df = getVector(text)\n",
    "print (\"Length of embeddings from Word2Vec embeddings:\",len(df.columns))\n",
    "print (\"====================================================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class 1 probability is 0.36 and class 0 probability is 0.63 , \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>so its likely fake </h1> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><font color=red> fake news </font> : <font color=red> \"republican says obama supports third trimester abortion\" </font> </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy in predicting fake news based on \n",
    "## Doc2Vec embeddings: 0.6410\n",
    "##  Word2Vec embeddings: 0.6416"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec embeddings with vector summation seems to perform similar to Doc2Vec embeddings\n",
    "# it could be because word2vec is generating vectors of size <u>100</u> and while doc2vec is generating vectors of size <u>20.</u> \n",
    "# Doc 2 vec seem to get the same accuracy with 20 embeddings that word2vec does with 100 embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
